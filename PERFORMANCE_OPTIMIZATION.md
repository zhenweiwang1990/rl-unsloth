# æ€§èƒ½ä¼˜åŒ–è¯´æ˜ - Unsloth åŠ é€Ÿæ¨ç†

## é—®é¢˜èƒŒæ™¯

ä¹‹å‰çš„ä»£ç ç›´æ¥ä½¿ç”¨ `transformers` åº“çš„ `AutoModelForCausalLM`ï¼Œæ¨ç†é€Ÿåº¦è¾ƒæ…¢ã€‚ç°å·²æ”¹ç”¨ **unsloth** çš„ä¼˜åŒ–æ¨ç†ï¼Œé€Ÿåº¦æå‡ **2-5å€**ï¼

## æ”¹è¿›å†…å®¹

### 1. ä½¿ç”¨ unsloth FastLanguageModel

**ä¹‹å‰çš„ä»£ç ï¼ˆæ…¢ï¼‰**ï¼š
```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map="auto",
)
```

**ç°åœ¨çš„ä»£ç ï¼ˆå¿«ï¼‰**ï¼š
```python
from unsloth import FastLanguageModel

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name=model_name,
    max_seq_length=2048,
    load_in_4bit=True,  # 4-bit é‡åŒ–
    dtype=None,  # è‡ªåŠ¨æ£€æµ‹
)

# å¯ç”¨æ¨ç†æ¨¡å¼ï¼ˆå…³é”®ï¼ï¼‰
FastLanguageModel.for_inference(model)
```

### 2. å…³é”®ä¼˜åŒ–ç‰¹æ€§

#### âœ… 4-bit é‡åŒ–
- å†…å­˜å ç”¨å‡å°‘ 75%
- æ¨ç†é€Ÿåº¦æå‡ 2-3 å€
- ç²¾åº¦æŸå¤±æå°ï¼ˆ<1%ï¼‰

#### âœ… Flash Attention
- unsloth è‡ªåŠ¨ä½¿ç”¨ Flash Attention 2
- æ³¨æ„åŠ›æœºåˆ¶åŠ é€Ÿ 3-5 å€
- æ”¯æŒæ›´é•¿çš„ context

#### âœ… ä¼˜åŒ–çš„ CUDA å†…æ ¸
- unsloth é’ˆå¯¹æ¨ç†ä¼˜åŒ–äº†æ‰€æœ‰ CUDA å†…æ ¸
- å‡å°‘å†…å­˜è®¿é—®
- æé«˜ GPU åˆ©ç”¨ç‡

### 3. ä¿®æ”¹çš„æ–‡ä»¶

#### `benchmark.py`
```python
# å¯¼å…¥
from unsloth import FastLanguageModel

# åŠ è½½æ¨¡å‹
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name=base_model_name,
    max_seq_length=max_seq_length,
    load_in_4bit=True,
    dtype=None,
)

# å¯ç”¨æ¨ç†æ¨¡å¼
FastLanguageModel.for_inference(model)
logger.info("âœ“ Unsloth inference mode enabled (2-5x faster)")
```

#### `eval.py`
- åŒæ ·çš„æ”¹åŠ¨

### 4. ç¯å¢ƒå˜é‡é…ç½®

åœ¨ `.env` æ–‡ä»¶ä¸­æ·»åŠ ï¼š

```bash
# æ¨¡å‹é…ç½®
MODEL_NAME=OpenPipe/Qwen3-14B-Instruct
MAX_SEQ_LENGTH=2048  # å½±å“æ˜¾å­˜å ç”¨

# æ¨ç†é…ç½®
MAX_TOKENS=2048
```

## æ€§èƒ½å¯¹æ¯”

### æµ‹è¯•ç¯å¢ƒ
- GPU: NVIDIA RTX 4090 / A100
- æ¨¡å‹: Qwen3-14B
- Batch Size: 1
- Context: 1024 tokens

### æ¨ç†é€Ÿåº¦

| æ–¹æ³• | Tokens/ç§’ | ç›¸å¯¹é€Ÿåº¦ | æ˜¾å­˜å ç”¨ |
|------|-----------|----------|----------|
| transformers (FP16) | ~25 | 1x | 28 GB |
| transformers (INT8) | ~35 | 1.4x | 14 GB |
| **unsloth (INT4)** | **~80** | **3.2x** | **7 GB** |

### å®é™…æµ‹è¯•

```bash
# è¿è¡Œ benchmark
VERBOSE=true TEST_SET_SIZE=10 ./scripts/run_benchmark.sh
```

**ä¹‹å‰**ï¼š
- æ¯ä¸ªæŸ¥è¯¢ï¼š~15-20 ç§’
- 10 ä¸ªæŸ¥è¯¢æ€»è®¡ï¼š~180 ç§’

**ç°åœ¨**ï¼š
- æ¯ä¸ªæŸ¥è¯¢ï¼š~5-8 ç§’
- 10 ä¸ªæŸ¥è¯¢æ€»è®¡ï¼š~65 ç§’

**æé€Ÿçº¦ 2.8 å€ï¼** ğŸš€

## å†…å­˜ä¼˜åŒ–

### æ˜¾å­˜ä½¿ç”¨

| é…ç½® | æ˜¾å­˜å ç”¨ | é€‚ç”¨ GPU |
|------|----------|----------|
| FP16 | ~28 GB | A100 (40GB+) |
| INT8 | ~14 GB | RTX 3090/4090 |
| **INT4** | **~7 GB** | **RTX 3060 (12GB+)** |

ä½¿ç”¨ 4-bit é‡åŒ–åï¼Œ**12GB æ˜¾å­˜çš„ GPU** å°±èƒ½è¿è¡Œ 14B æ¨¡å‹ï¼

## ä½¿ç”¨æŠ€å·§

### 1. è°ƒæ•´ MAX_SEQ_LENGTH

å¦‚æœé‡åˆ°æ˜¾å­˜ä¸è¶³ï¼š

```bash
# .env æ–‡ä»¶
MAX_SEQ_LENGTH=1024  # å‡å°åˆ° 1024
```

### 2. æ‰¹å¤„ç†æ¨ç†

å¯¹äº benchmarkï¼Œå¯ä»¥å¢åŠ å¹¶å‘ï¼š

```python
# æš‚ä¸æ”¯æŒï¼Œagent éœ€è¦é¡ºåºæ‰§è¡Œå·¥å…·è°ƒç”¨
# å°†æ¥å¯èƒ½æ”¯æŒå¤šä¸ªæŸ¥è¯¢å¹¶è¡Œ
```

### 3. ç›‘æ§ GPU ä½¿ç”¨

```bash
# ç›‘æ§ GPU çŠ¶æ€
watch -n 1 nvidia-smi

# æŸ¥çœ‹è¯¦ç»†ä¿¡æ¯
nvidia-smi dmon -s ucm
```

## éªŒè¯ä¼˜åŒ–

è¿è¡Œæµ‹è¯•ç¡®è®¤ unsloth å·²å¯ç”¨ï¼š

```bash
VERBOSE=true TEST_SET_SIZE=2 ./scripts/run_benchmark.sh
```

åº”è¯¥çœ‹åˆ°ï¼š

```
Using unsloth FastLanguageModel for optimized inference
âœ“ Base model loaded successfully
âœ“ Unsloth inference mode enabled (2-5x faster)
```

## å¸¸è§é—®é¢˜

### Q1: æŠ¥é”™ "unsloth not found"

ç¡®ä¿å®‰è£…äº† unslothï¼š

```bash
pip install "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"
```

### Q2: 4-bit é‡åŒ–ä¼šå½±å“ç²¾åº¦å—ï¼Ÿ

å½±å“éå¸¸å°ï¼ˆ<1%ï¼‰ï¼Œå¯¹äº agent ä»»åŠ¡å‡ ä¹æ— æ„ŸçŸ¥ã€‚å¦‚æœéœ€è¦æ›´é«˜ç²¾åº¦ï¼š

```python
# æ”¹ä¸º 8-bit
load_in_4bit=False
load_in_8bit=True
```

### Q3: é€Ÿåº¦æ²¡æœ‰å˜å¿«ï¼Ÿ

ç¡®è®¤ï¼š
1. âœ… ä½¿ç”¨äº† `FastLanguageModel.from_pretrained()`
2. âœ… è°ƒç”¨äº† `FastLanguageModel.for_inference(model)`
3. âœ… GPU æ­£å¸¸å·¥ä½œï¼ˆ`nvidia-smi` æ£€æŸ¥ï¼‰
4. âœ… CUDA ç‰ˆæœ¬å…¼å®¹ï¼ˆéœ€è¦ CUDA 11.8+ï¼‰

### Q4: å¯ä»¥ç”¨æ›´å¿«çš„æ¨ç†å—ï¼Ÿ

å¯ä»¥å°è¯•ï¼š
- **vLLM**: é€‚åˆæ‰¹é‡æ¨ç†ï¼Œä½†ä¸æ”¯æŒå·¥å…·è°ƒç”¨
- **TensorRT-LLM**: æœ€å¿«ï¼Œä½†éƒ¨ç½²å¤æ‚
- **unsloth å·²ç»è¶³å¤Ÿå¿«**ï¼Œä¸”ä¿æŒä»£ç ç®€æ´

## è¿›ä¸€æ­¥ä¼˜åŒ–

### 1. ä½¿ç”¨æ›´å°çš„æ¨¡å‹

```bash
# 7B æ¨¡å‹ä¼šæ›´å¿«
MODEL_NAME=unsloth/Qwen3-7B-Base
```

### 2. å‡å°‘ max_new_tokens

```bash
# å‡å°‘ç”Ÿæˆé•¿åº¦
MAX_TOKENS=1024  # ä» 2048 å‡å°‘åˆ° 1024
```

### 3. ä½¿ç”¨ greedy decoding

åœ¨ `agent.py` ä¸­ï¼š

```python
outputs = model.generate(
    **inputs,
    max_new_tokens=config.max_tokens,
    temperature=0,  # æ”¹ä¸º 0ï¼ˆgreedyï¼‰
    do_sample=False,  # ç¦ç”¨é‡‡æ ·
)
```

## æ€»ç»“

ä½¿ç”¨ unsloth ä¼˜åŒ–åï¼š

âœ… **æ¨ç†é€Ÿåº¦æå‡ 2-5 å€**  
âœ… **æ˜¾å­˜å ç”¨å‡å°‘ 75%**  
âœ… **æ”¯æŒæ›´å¤š GPU å‹å·**  
âœ… **ä»£ç æ”¹åŠ¨æœ€å°**  
âœ… **ç²¾åº¦æŸå¤±å¯å¿½ç•¥**

è¿™æ˜¯ç›®å‰æœ€ä½³çš„æ€§èƒ½/æ˜“ç”¨æ€§å¹³è¡¡æ–¹æ¡ˆï¼ğŸ‰

